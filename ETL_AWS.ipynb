{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-6239MhN_x_v",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-6239MhN_x_v",
    "outputId": "3618de6b-5bf9-4e8e-c225-dcef0ee9174e"
   },
   "outputs": [],
   "source": [
    "!pip install pandas numpy pyspark findspark boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f1ed70-f0d0-41ff-8bf1-a4019a6e417a",
   "metadata": {
    "id": "d6f1ed70-f0d0-41ff-8bf1-a4019a6e417a"
   },
   "outputs": [],
   "source": [
    "#Importing all necessary libraries\n",
    "\n",
    "import pandas, numpy, os\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import boto3\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b642a6d2-e393-48f2-b54a-a83a5e68a5d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b642a6d2-e393-48f2-b54a-a83a5e68a5d3",
    "outputId": "8060a1c6-9d77-453c-9c75-e3f789d416c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.0.3\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.10, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_321\n",
      "Branch HEAD\n",
      "Compiled by user ubuntu on 2021-06-17T04:08:22Z\n",
      "Revision 65ac1e75dc468f53fc778cd2ce1ba3f21067aab8\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aee00af-cf7f-44c7-98f5-3ae84adc48d0",
   "metadata": {
    "id": "8aee00af-cf7f-44c7-98f5-3ae84adc48d0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkConf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\PURUSH~1\\AppData\\Local\\Temp/ipykernel_8328/1631256224.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Creating a Spark Session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Purush_ETL\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local[2]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SparkConf' is not defined"
     ]
    }
   ],
   "source": [
    "#Creating a Spark Session\n",
    "\n",
    "conf = SparkConf()\\\n",
    "    .setAppName(\"Purush_ETL\")\\\n",
    "    .setMaster(\"local[2]\")\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config(conf = conf)\\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\")\\\n",
    "    .config(\"spark.executor.cores\", \"5\")\\\n",
    "    .config(\"spark.driver.memory\", \"2g\")\\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\",\"1\")\\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\",\"3\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18df0cda-94a1-459a-8c24-1a7773e62088",
   "metadata": {
    "id": "18df0cda-94a1-459a-8c24-1a7773e62088"
   },
   "outputs": [],
   "source": [
    "pathforDWH = \"\"\n",
    "s3_bucket_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iXvXoRTeBXm6",
   "metadata": {
    "id": "iXvXoRTeBXm6"
   },
   "outputs": [],
   "source": [
    "# Configure AWS credentials\n",
    "aws_access_key_id = \"\"\n",
    "aws_secret_access_key = \"\"\n",
    "\n",
    "# Set up the AWS session\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id = aws_access_key_id,\n",
    "    aws_secret_access_key = aws_secret_access_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c232f51-5864-4638-8a2b-338816a23c92",
   "metadata": {},
   "source": [
    "## Reading the data from S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a168c7-3776-4143-92dc-ecc913fa6f0f",
   "metadata": {},
   "source": [
    "### Reading the Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8043bba3-1712-4d4c-a5f5-d0056a257e44",
   "metadata": {
    "id": "8043bba3-1712-4d4c-a5f5-d0056a257e44"
   },
   "outputs": [],
   "source": [
    "sym_meta = spark.read.options(\"delimiter\": \",\").csv(\"symbol_metadata.csv\")\n",
    "\n",
    "sym_meta.printSchema() #to check the Schema of the dataframe\n",
    "#sym_meta.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8f52b2-4317-40f9-8b14-bc5cc8319eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before reading the Stock data, we need to make sure we read the Stock data  \n",
    "## only for those companies/symbols that are listed in the Metadata file.\n",
    "sym_list = [x for x in sym_meta[\"Symbol\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d556a6e-95f1-4736-87b8-2702264cec0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reading the Stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36378a57-4d2e-4c14-bf2f-549bac6dd6b3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "36378a57-4d2e-4c14-bf2f-549bac6dd6b3",
    "outputId": "224a8380-b6b9-4abd-ab9d-bf785335e49a"
   },
   "outputs": [],
   "source": [
    "stock_data = spark.read.options(\"delimiter\": \",\").csv([s3_bucket_path+\"/\"+ x for x in sym_list])\n",
    "\n",
    "stock_data.printSchema() #to check the Schema of the dataframe\n",
    "#stock_data.show()\n",
    "\n",
    "stock_data.withColumn(\"Symbol\", input_file_name()).repartition(col(\"Symbol\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbf6765",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data_full = stock_data.join(sym_meta, stock_data[\"Symbol\"] == sym_meta[\"Symbol\"], \"left\")\\\n",
    "    .select(stock_data['*'], \n",
    "            sym_meta[\"Symbol\"],\n",
    "            sym_meta[\"Name\"],\n",
    "            sym_meta[\"Country\"],\n",
    "            sym_meta[\"Sector\"],\n",
    "            sym_meta[\"Industry\"])\n",
    "\n",
    "stock_data_full.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97ef8e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b45ebbc7",
   "metadata": {},
   "source": [
    "### Summary Report (All Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3102162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_report_all_func(stock_data_full, industries):\n",
    "    summary_report_output__all_time = stock_data_full.filter(stock_data_full[\"Sector\"].isin(industries)) \\\n",
    "        .groupBy(stock_data_full[\"Sector\"])\\\n",
    "        .agg(\n",
    "            avg(stock_data_full[\"open\"]).alias(\"Avg Open Price\"), \\\n",
    "            avg(stock_data_full[\"close\"]).alias(\"Avg Close Price\"), \\\n",
    "            max(stock_data_full[\"high\"]).alias(\"Max High Price\"), \\\n",
    "            min(stock_data_full[\"low\"]).alias(\"Min Low Price\"), \\\n",
    "            avg(stock_data_full[\"volume\"]).alias(\"Avg Volume\") \\\n",
    "        )\n",
    "    \n",
    "    summary_report_output__all_time.printSchema()\n",
    "\n",
    "    #For developmental purposes I'm trying to store the data in a Dataframe and then return it; rather than directly returning it.\n",
    "    return summary_report_output__all_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be17156",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_industries = int(input(\"Enter the number of industries for which you wanted Summary Report (All Time):\"))\n",
    "arr = input()   # takes the whole line of no_industries strings\n",
    "industries = list(arr.split(',')) # split those strings with ','\n",
    "\n",
    "summary_report_all_func(stock_data_full, industries).show(n = len(sym_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c986d8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2536efc5",
   "metadata": {},
   "source": [
    "### Summary Report (Period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f970469b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_report_period_func(stock_data_full, sectors, start_date, end_date):\n",
    "    summary_report_output__period = stock_data_full \\\n",
    "        .filter(stock_data_full[\"Sector\"].isin(sectors), \n",
    "                to_date(stock_data_full[\"timestamp\"], \"YYYY-MM-DD\").between(start_date, end_date)\n",
    "        ) \\\n",
    "        .groupBy(stock_data_full[\"Sector\"]) \\\n",
    "        .agg(\n",
    "            avg(stock_data_full[\"open\"]).alias(\"Avg Open Price\"), \\\n",
    "            avg(stock_data_full[\"close\"]).alias(\"Avg Close Price\"), \\\n",
    "            max(stock_data_full[\"high\"]).alias(\"Max High Price\"), \\\n",
    "            min(stock_data_full[\"low\"]).alias(\"Min Low Price\"), \\\n",
    "            avg(stock_data_full[\"volume\"]).alias(\"Avg Volume\") \\\n",
    "        )\n",
    "\n",
    "    #For developmental purposes I'm trying to store the data in a Dataframe and then return it; rather than directly returning it.\n",
    "    return summary_report_output__period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856ccb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = to_date(input(\"Enter the start date of the period for Summary Report [YYYY-MM-DD]:\"), \"YYYY-MM-DD\")\n",
    "end_date = to_date(input(\"Enter the end date of the period for Summary Report [YYYY-MM-DD]:\"), \"YYYY-MM-DD\")\n",
    "no_sectors = int(input(\"Enter the number of sectors for which you wanted Summary Report (Given Period):\"))\n",
    "arr = input()   # takes the whole line of no_sectors strings\n",
    "sectors = list(arr.split(',')) # split those strings with ','\n",
    "\n",
    "summary_report_period_func(stock_data_full, sectors, start_date, end_date).show(n = len(sym_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473e4c35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "355f96a6",
   "metadata": {},
   "source": [
    "### Detailed Reports (Period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba0c2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_report_period_func(stock_data_full, sectors, start_date, end_date):\n",
    "    detailed_report_output__period = stock_data_full \\\n",
    "        .filter(stock_data_full[\"Sector\"].isin(sectors), \n",
    "                to_date(stock_data_full[\"timestamp\"], \"YYYY-MM-DD\").between(start_date, end_date)\n",
    "        ) \\\n",
    "        .groupBy(stock_data_full[\"Symbol\"], stock_data_full[\"Name\"]) \\\n",
    "        .agg(\n",
    "            avg(stock_data_full[\"open\"]).alias(\"Avg Open Price\"), \\\n",
    "            avg(stock_data_full[\"close\"]).alias(\"Avg Close Price\"), \\\n",
    "            max(stock_data_full[\"high\"]).alias(\"Max High Price\"), \\\n",
    "            min(stock_data_full[\"low\"]).alias(\"Min Low Price\"), \\\n",
    "            avg(stock_data_full[\"volume\"]).alias(\"Avg Volume\") \\\n",
    "        )\n",
    "\n",
    "    #For developmental purposes I'm trying to store the data in a Dataframe and then return it; rather than directly returning it.\n",
    "    detailed_report_output__period.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f3709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = to_date(input(\"Enter the start date of the period for Detailed Reports [YYYY-MM-DD]:\"), \"YYYY-MM-DD\")\n",
    "end_date = to_date(input(\"Enter the end date of the period for Detailed Reports [YYYY-MM-DD]:\"), \"YYYY-MM-DD\")\n",
    "no_sectors = int(input(\"Enter the number of sectors for which you wanted Detailed Report (Given Period):\"))\n",
    "arr = input()   # takes the whole line of no_sectors strings\n",
    "sectors = list(arr.split(',')) # split those strings with ','\n",
    "\n",
    "detailed_report_output__period(stock_data_full, sectors, start_date, end_date).show(n = len(sym_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5462911a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f7746a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92533a94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd25766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef739f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90488ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46661e8b-4f6e-4704-b085-ee07b20a3397",
   "metadata": {
    "id": "46661e8b-4f6e-4704-b085-ee07b20a3397"
   },
   "outputs": [],
   "source": [
    "df.repartion('Month').write.mode(\"overwrite\").format(\"parquet\").path(pathforDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a259a4-1ede-4681-adce-14f158cc9c58",
   "metadata": {
    "id": "34a259a4-1ede-4681-adce-14f158cc9c58"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e7cd5b-347c-4dd7-893a-5fd16c285d69",
   "metadata": {
    "id": "b5e7cd5b-347c-4dd7-893a-5fd16c285d69"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4797de32-3b77-47da-8e33-c387c21928c7",
   "metadata": {
    "id": "4797de32-3b77-47da-8e33-c387c21928c7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0679aa-08cc-4952-81ec-8fbe29eacd5e",
   "metadata": {
    "id": "5d0679aa-08cc-4952-81ec-8fbe29eacd5e"
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
