{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "-6239MhN_x_v",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-6239MhN_x_v",
    "outputId": "3618de6b-5bf9-4e8e-c225-dcef0ee9174e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy pyspark findspark boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6f1ed70-f0d0-41ff-8bf1-a4019a6e417a",
   "metadata": {
    "id": "d6f1ed70-f0d0-41ff-8bf1-a4019a6e417a"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\PURUSH~1\\AppData\\Local\\Temp/ipykernel_24016/461930472.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "#Importing all necessary libraries\n",
    "\n",
    "import pandas, numpy, os\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import boto3\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42282a9c-130b-44ad-889d-4ca7fc2e9b7c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_UjZZH7BFEoR",
    "outputId": "a50ed13c-897b-462b-deb3-cb6d2db96850"
   },
   "source": [
    "!sudo wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.11.30/aws-java-sdk-1.11.30.jar\n",
    "!sudo wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar\n",
    "!sudo wget https://repo1.maven.org/maven2/net/java/dev/jets3t/jets3t/0.9.4/jets3t-0.9.4.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c68db5-a815-4721-aa86-5bf027f3de3c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SqWiWlIdCcZb",
    "outputId": "6cf0a5c2-f4ec-48a6-beb6-a51da032f2f1"
   },
   "source": [
    "# Activate Spark in our Colab notebook.\n",
    "import os\n",
    "# Find the latest version of spark 3.2  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example:\n",
    "# spark_version = 'spark-3.2.2'\n",
    "spark_version = 'spark-3.2.3'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b642a6d2-e393-48f2-b54a-a83a5e68a5d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b642a6d2-e393-48f2-b54a-a83a5e68a5d3",
    "outputId": "8060a1c6-9d77-453c-9c75-e3f789d416c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.0.3\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.10, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_321\n",
      "Branch HEAD\n",
      "Compiled by user ubuntu on 2021-06-17T04:08:22Z\n",
      "Revision 65ac1e75dc468f53fc778cd2ce1ba3f21067aab8\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cc3fce-4d27-428f-abcb-0c04292a707b",
   "metadata": {},
   "source": [
    "export SPARK_HOME=/Users/prabha/apps/spark-2.4.0-bin-hadoop2.7\n",
    "export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/build:$PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aee00af-cf7f-44c7-98f5-3ae84adc48d0",
   "metadata": {
    "id": "8aee00af-cf7f-44c7-98f5-3ae84adc48d0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkConf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\PURUSH~1\\AppData\\Local\\Temp/ipykernel_8328/1631256224.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Creating a Spark Session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Purush_ETL\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local[2]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SparkConf' is not defined"
     ]
    }
   ],
   "source": [
    "#Creating a Spark Session\n",
    "\n",
    "conf = SparkConf()\\\n",
    "    .setAppName(\"Purush_ETL\")\\\n",
    "    .setMaster(\"local[2]\")\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config(conf = conf)\\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\")\\\n",
    "    .config(\"spark.executor.cores\", \"5\")\\\n",
    "    .config(\"spark.driver.memory\", \"2g\")\\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\",\"1\")\\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\",\"3\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18df0cda-94a1-459a-8c24-1a7773e62088",
   "metadata": {
    "id": "18df0cda-94a1-459a-8c24-1a7773e62088"
   },
   "outputs": [],
   "source": [
    "pathforDWH = \"\"\n",
    "s3_bucket_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iXvXoRTeBXm6",
   "metadata": {
    "id": "iXvXoRTeBXm6"
   },
   "outputs": [],
   "source": [
    "# Configure AWS credentials\n",
    "aws_access_key_id = \"\"\n",
    "aws_secret_access_key = \"\"\n",
    "\n",
    "# Set up the AWS session\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id = aws_access_key_id,\n",
    "    aws_secret_access_key = aws_secret_access_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c232f51-5864-4638-8a2b-338816a23c92",
   "metadata": {},
   "source": [
    "## Reading the data from S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a168c7-3776-4143-92dc-ecc913fa6f0f",
   "metadata": {},
   "source": [
    "### Reading the Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8043bba3-1712-4d4c-a5f5-d0056a257e44",
   "metadata": {
    "id": "8043bba3-1712-4d4c-a5f5-d0056a257e44"
   },
   "outputs": [],
   "source": [
    "sym_meta = spark.read.options(\"delimiter\": \",\").csv(\"symbol_metadata.csv\")\n",
    "\n",
    "sym_meta.printSchema() #to check the Schema of the dataframe\n",
    "#sym_meta.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccb5f6b-d24f-4a18-a2d7-655b4db17f41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8f52b2-4317-40f9-8b14-bc5cc8319eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before reading the Stock data, we need to make sure we read the Stock data  \n",
    "## only for those companies/symbols that are listed in the Metadata file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f534b440-2cd3-4ca6-a04b-d78b7217f0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sym_list = [x for x in sym_meta[\"Symbol\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d556a6e-95f1-4736-87b8-2702264cec0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reading the Stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36378a57-4d2e-4c14-bf2f-549bac6dd6b3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "36378a57-4d2e-4c14-bf2f-549bac6dd6b3",
    "outputId": "224a8380-b6b9-4abd-ab9d-bf785335e49a"
   },
   "outputs": [],
   "source": [
    "stock_data = spark.read.options(\"delimiter\": \",\").csv([s3_bucket_path+\"/\"+ x for x in sym_list])\n",
    "\n",
    "stock_data.printSchema() #to check the Schema of the dataframe\n",
    "#stock_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d193ceb4-ce67-491b-9961-a8c0c2b40f52",
   "metadata": {
    "id": "d193ceb4-ce67-491b-9961-a8c0c2b40f52"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46661e8b-4f6e-4704-b085-ee07b20a3397",
   "metadata": {
    "id": "46661e8b-4f6e-4704-b085-ee07b20a3397"
   },
   "outputs": [],
   "source": [
    "df.repartion('Month').write.mode(\"overwrite\").format(\"parquet\").path(pathforDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a259a4-1ede-4681-adce-14f158cc9c58",
   "metadata": {
    "id": "34a259a4-1ede-4681-adce-14f158cc9c58"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e7cd5b-347c-4dd7-893a-5fd16c285d69",
   "metadata": {
    "id": "b5e7cd5b-347c-4dd7-893a-5fd16c285d69"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4797de32-3b77-47da-8e33-c387c21928c7",
   "metadata": {
    "id": "4797de32-3b77-47da-8e33-c387c21928c7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0679aa-08cc-4952-81ec-8fbe29eacd5e",
   "metadata": {
    "id": "5d0679aa-08cc-4952-81ec-8fbe29eacd5e"
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
