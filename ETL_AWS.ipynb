{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-6239MhN_x_v",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-6239MhN_x_v",
    "outputId": "3618de6b-5bf9-4e8e-c225-dcef0ee9174e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=ab49ab6e8a856e57c8fcf681d395a24894abac99b4efd3ca33ed7f25e17ab363\n",
      "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
      "Successfully built pyspark\n",
      "Installing collected packages: findspark, pyspark\n",
      "Successfully installed findspark-2.0.1 pyspark-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy pyspark findspark boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6f1ed70-f0d0-41ff-8bf1-a4019a6e417a",
   "metadata": {
    "id": "d6f1ed70-f0d0-41ff-8bf1-a4019a6e417a"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\PURUSH~1\\AppData\\Local\\Temp/ipykernel_8328/461930472.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "#Importing all necessary libraries\n",
    "\n",
    "import pandas, numpy, os\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import boto3\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42282a9c-130b-44ad-889d-4ca7fc2e9b7c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_UjZZH7BFEoR",
    "outputId": "a50ed13c-897b-462b-deb3-cb6d2db96850"
   },
   "source": [
    "!sudo wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.11.30/aws-java-sdk-1.11.30.jar\n",
    "!sudo wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar\n",
    "!sudo wget https://repo1.maven.org/maven2/net/java/dev/jets3t/jets3t/0.9.4/jets3t-0.9.4.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c68db5-a815-4721-aa86-5bf027f3de3c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SqWiWlIdCcZb",
    "outputId": "6cf0a5c2-f4ec-48a6-beb6-a51da032f2f1"
   },
   "source": [
    "# Activate Spark in our Colab notebook.\n",
    "import os\n",
    "# Find the latest version of spark 3.2  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example:\n",
    "# spark_version = 'spark-3.2.2'\n",
    "spark_version = 'spark-3.2.3'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b642a6d2-e393-48f2-b54a-a83a5e68a5d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b642a6d2-e393-48f2-b54a-a83a5e68a5d3",
    "outputId": "8060a1c6-9d77-453c-9c75-e3f789d416c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.0.3\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.10, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_321\n",
      "Branch HEAD\n",
      "Compiled by user ubuntu on 2021-06-17T04:08:22Z\n",
      "Revision 65ac1e75dc468f53fc778cd2ce1ba3f21067aab8\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "886aa13c-67fa-4c8c-b0a7-24922fef38b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_8328/3505754490.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\PURUSH~1\\AppData\\Local\\Temp/ipykernel_8328/3505754490.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    export SPARK_HOME=/Users/prabha/apps/spark-2.4.0-bin-hadoop2.7\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "export SPARK_HOME=/Users/prabha/apps/spark-2.4.0-bin-hadoop2.7\n",
    "export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/build:$PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aee00af-cf7f-44c7-98f5-3ae84adc48d0",
   "metadata": {
    "id": "8aee00af-cf7f-44c7-98f5-3ae84adc48d0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkConf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\PURUSH~1\\AppData\\Local\\Temp/ipykernel_8328/1631256224.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Creating a Spark Session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Purush_ETL\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local[2]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SparkConf' is not defined"
     ]
    }
   ],
   "source": [
    "#Creating a Spark Session\n",
    "\n",
    "conf = SparkConf()\\\n",
    "    .setAppName(\"Purush_ETL\")\\\n",
    "    .setMaster(\"local[2]\")\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config(conf = conf)\\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\")\\\n",
    "    .config(\"spark.executor.cores\", \"5\")\\\n",
    "    .config(\"spark.driver.memory\", \"2g\")\\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\",\"1\")\\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\",\"3\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18df0cda-94a1-459a-8c24-1a7773e62088",
   "metadata": {
    "id": "18df0cda-94a1-459a-8c24-1a7773e62088"
   },
   "outputs": [],
   "source": [
    "pathforDWH = \"\"\n",
    "s3_bucket_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iXvXoRTeBXm6",
   "metadata": {
    "id": "iXvXoRTeBXm6"
   },
   "outputs": [],
   "source": [
    "# Configure AWS credentials\n",
    "aws_access_key_id = \"\"\n",
    "aws_secret_access_key = \"\"\n",
    "\n",
    "# Set up the AWS session\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id = aws_access_key_id,\n",
    "    aws_secret_access_key = aws_secret_access_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c232f51-5864-4638-8a2b-338816a23c92",
   "metadata": {},
   "source": [
    "## Reading the data from S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a168c7-3776-4143-92dc-ecc913fa6f0f",
   "metadata": {},
   "source": [
    "### Reading the Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8043bba3-1712-4d4c-a5f5-d0056a257e44",
   "metadata": {
    "id": "8043bba3-1712-4d4c-a5f5-d0056a257e44"
   },
   "outputs": [],
   "source": [
    "sym_meta = spark.read.options(\"delimiter\": \",\").csv(\"symbol_metadata.csv\")\n",
    "\n",
    "sym_meta.printSchema() #to check the Schema of the dataframe\n",
    "#sym_meta.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccb5f6b-d24f-4a18-a2d7-655b4db17f41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8f52b2-4317-40f9-8b14-bc5cc8319eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before reading the Stock data, we need to make sure we read the Stock data  \n",
    "## only for those companies/symbols that are listed in the Metadata file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f534b440-2cd3-4ca6-a04b-d78b7217f0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sym_list = [x for x in sym_meta[\"Symbol\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d556a6e-95f1-4736-87b8-2702264cec0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reading the Stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36378a57-4d2e-4c14-bf2f-549bac6dd6b3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "36378a57-4d2e-4c14-bf2f-549bac6dd6b3",
    "outputId": "224a8380-b6b9-4abd-ab9d-bf785335e49a"
   },
   "outputs": [],
   "source": [
    "stock_data = spark.read.options(\"delimiter\": \",\").csv([s3_bucket_path+\"/\"+ x for x in sym_list])\n",
    "\n",
    "stock_data.printSchema() #to check the Schema of the dataframe\n",
    "#stock_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d193ceb4-ce67-491b-9961-a8c0c2b40f52",
   "metadata": {
    "id": "d193ceb4-ce67-491b-9961-a8c0c2b40f52"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46661e8b-4f6e-4704-b085-ee07b20a3397",
   "metadata": {
    "id": "46661e8b-4f6e-4704-b085-ee07b20a3397"
   },
   "outputs": [],
   "source": [
    "df.repartion('Month').write.mode(\"overwrite\").format(\"parquet\").path(pathforDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a259a4-1ede-4681-adce-14f158cc9c58",
   "metadata": {
    "id": "34a259a4-1ede-4681-adce-14f158cc9c58"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e7cd5b-347c-4dd7-893a-5fd16c285d69",
   "metadata": {
    "id": "b5e7cd5b-347c-4dd7-893a-5fd16c285d69"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4797de32-3b77-47da-8e33-c387c21928c7",
   "metadata": {
    "id": "4797de32-3b77-47da-8e33-c387c21928c7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0679aa-08cc-4952-81ec-8fbe29eacd5e",
   "metadata": {
    "id": "5d0679aa-08cc-4952-81ec-8fbe29eacd5e"
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
